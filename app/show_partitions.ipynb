{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparando ambiente para PySpark"
      ],
      "metadata": {
        "id": "lweR8KexAd8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dependências para Python 3.10"
      ],
      "metadata": {
        "id": "vQh272zRA2Q2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gnh9h7R01pl0"
      },
      "outputs": [],
      "source": [
        "# instalar as dependências\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Configuração das Variáveis de Ambiente"
      ],
      "metadata": {
        "id": "PqZqZAKQA8jR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configurar as variáveis de ambiente\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "YFDKyVhD15h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tornar PySpark Importável"
      ],
      "metadata": {
        "id": "aXbt-dX5BBvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tornar o pyspark \"importável\"\n",
        "import findspark\n",
        "findspark.init('spark-3.4.0-bin-hadoop3')"
      ],
      "metadata": {
        "id": "OVpfT2Fv8pZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iniciando Ambiente"
      ],
      "metadata": {
        "id": "gGNgfFFcAjQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iniciar uma sessão local e importar dados do Airbnb\n",
        "from pyspark.sql import SparkSession\n",
        "sc = SparkSession.builder.master('local[*]').getOrCreate()"
      ],
      "metadata": {
        "id": "M3Y7J0Sm8wd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Carregando dados"
      ],
      "metadata": {
        "id": "L7Wk-av6AlcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Faça Upload do seu Arquivo\n",
        "#from google.colab import files\n",
        "#f = files.upload()"
      ],
      "metadata": {
        "id": "W61s0Oxx2An4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "i4hjNAeT-BEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_pd = pd.read_excel('Data.xlsx')\n",
        "#Carregando dados\n",
        "#df_spark = sc.read.csv(\"train.csv\", inferSchema=True, header=True)\n",
        "#df_sp = sc.createDataFrame(df_pd)"
      ],
      "metadata": {
        "id": "ddBhOyoN8dsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame.from_dict({\n",
        "    'partitions': [\n",
        "        'ano=2023/mes=04/dia=24/sigla=\"EY4\"',\n",
        "        'ano=2024/mes=04/dia=23/sigla=\"EY4\"',\n",
        "        'ano=2024/mes=04/dia=22/sigla=\"EY4\"',\n",
        "        'ano=2024/mes=04/dia=21/sigla=\"EY4\"',\n",
        "        'ano=2024/mes=04/dia=20/sigla=\"EY4\"',\n",
        "        'ano=2024/mes=04/dia=19/sigla=\"EY4\"',\n",
        "        'ano=2024/mes=04/dia=18/sigla=\"EY4\"',\n",
        "        'ano=2024/mes=04/dia=17/sigla=\"EY4\"',\n",
        "        'ano=2024/mes=04/dia=16/sigla=\"EY4\"',\n",
        "        'ano=2024/mes=03/dia=15/sigla=\"EY4\"',\n",
        "        'ano=2024/mes=03/dia=05/sigla=\"EY4\"',\n",
        "        'ano=2024/mes=03/dia=04/sigla=\"EY4\"',\n",
        "        'ano=2024/mes=03/dia=03/sigla=\"EY4\"',\n",
        "        'ano=2024/mes=03/dia=02/sigla=\"EY4\"',\n",
        "    ]\n",
        "})"
      ],
      "metadata": {
        "id": "i9KsWu-3uiqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = sc.createDataFrame(df)"
      ],
      "metadata": {
        "id": "DPV5xQ7FvNCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funções Basicas"
      ],
      "metadata": {
        "id": "_-dSFWz-Arna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Schema"
      ],
      "metadata": {
        "id": "LrrWuPDvDP4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ver algumas informações sobre os tipos de dados de cada coluna\n",
        "dfs.printSchema()"
      ],
      "metadata": {
        "id": "BegoXWTDAqzO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b5894a9-6111-4093-a83a-e5a4340da874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- partitions: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mostrar primeiras linhas"
      ],
      "metadata": {
        "id": "Id6rf2oXDTMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfs.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmTAMGyR2gPh",
        "outputId": "f4d8c5cd-9f91-4780-c3fd-72629eaeab5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|          partitions|\n",
            "+--------------------+\n",
            "|ano=2023/mes=04/d...|\n",
            "|ano=2024/mes=04/d...|\n",
            "|ano=2024/mes=04/d...|\n",
            "|ano=2024/mes=04/d...|\n",
            "|ano=2024/mes=04/d...|\n",
            "|ano=2024/mes=04/d...|\n",
            "|ano=2024/mes=04/d...|\n",
            "|ano=2024/mes=04/d...|\n",
            "|ano=2024/mes=04/d...|\n",
            "|ano=2024/mes=03/d...|\n",
            "+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split\n",
        "#column_name = split(dfs.partitions, \"=\").getItem(0)\n",
        "qnt_partitions = len(dfs.first()[0].split('/'))\n",
        "columns_names = dfs.first()[0].split('/')\n",
        "for partition in range(qnt_partitions):\n",
        "    dfs = dfs.withColumn(columns_names[partition].split('=')[0], split(split(dfs.partitions, '/').getItem(partition),'=').getItem(1))"
      ],
      "metadata": {
        "id": "WiNVKg88vdDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs.show(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne4Rws_rz4rt",
        "outputId": "87ce9503-82df-4d6c-bb53-10edd9cab4c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----+---+---+-----+\n",
            "|          partitions| ano|mes|dia|sigla|\n",
            "+--------------------+----+---+---+-----+\n",
            "|ano=2023/mes=04/d...|2023| 04| 24|\"EY4\"|\n",
            "+--------------------+----+---+---+-----+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "partitions_columns = dfs.columns\n",
        "dfs = dfs.drop(partitions_columns[0])\n",
        "dfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vad4XokbR86f",
        "outputId": "58f39eab-9288-498c-d045-484178de2a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[ano: string, mes: string, dia: string, sigla: string]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort by \"salary\" in descending order\n",
        "dfs.orderBy(dfs.columns, ascending=[False for column in dfs.columns]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V26ToV7iUE3y",
        "outputId": "b9ed5912-cf3c-4936-8abf-d8de220d9ef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+---+-----+\n",
            "| ano|mes|dia|sigla|\n",
            "+----+---+---+-----+\n",
            "|2024| 04| 23|\"EY4\"|\n",
            "|2024| 04| 22|\"EY4\"|\n",
            "|2024| 04| 21|\"EY4\"|\n",
            "|2024| 04| 20|\"EY4\"|\n",
            "|2024| 04| 19|\"EY4\"|\n",
            "|2024| 04| 18|\"EY4\"|\n",
            "|2024| 04| 17|\"EY4\"|\n",
            "|2024| 04| 16|\"EY4\"|\n",
            "|2024| 03| 15|\"EY4\"|\n",
            "|2024| 03| 05|\"EY4\"|\n",
            "|2024| 03| 04|\"EY4\"|\n",
            "|2024| 03| 03|\"EY4\"|\n",
            "|2024| 03| 02|\"EY4\"|\n",
            "|2023| 04| 24|\"EY4\"|\n",
            "+----+---+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for predicate in dfs.first():\n",
        "    print(predicate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSz3DmnkYYzY",
        "outputId": "363b754a-4584-4717-af7c-d2cec87b346f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023\n",
            "04\n",
            "24\n",
            "\"EY4\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from pyspark.sql.functions import col, concat, cast, lit, to_date, unix_timestamp, from_unixtime"
      ],
      "metadata": {
        "id": "lokXjLihfF3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "partitions = {\n",
        "    'ano':\"%Y\",\n",
        "    'mes':\"%m\",\n",
        "    'dia':\"%d\",\n",
        "}\n",
        "#dfs = dfs.withColumn(\"data_concatenada\", to_date(concat(dfs.ano, lit(\"-\"), dfs.mes, lit(\"-\"), dfs.dia),'yyyy-MM-dd'))\n",
        "# concat(dfs.ano, lit(\"-\"), dfs.mes, lit(\"-\"), dfs.dia) => 'yyyy-MM-dd'\n",
        "dfs = dfs.withColumn(\n",
        "    \"data_formatada\"\n",
        "    ,from_unixtime(\n",
        "        unix_timestamp(\n",
        "            concat(dfs.ano, dfs.mes, dfs.dia)\n",
        "            , \"yyyyMMdd\"\n",
        "        )\n",
        "        ,\"yyyy-MM-dd\"\n",
        "    )\n",
        ")\n",
        "dfs.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcaTkVUcPvdN",
        "outputId": "c96c6359-dbe4-4694-8a48-d3a3ab3c9c15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+---+-----+--------------+\n",
            "| ano|mes|dia|sigla|data_formatada|\n",
            "+----+---+---+-----+--------------+\n",
            "|2023| 04| 24|\"EY4\"|    2023-04-24|\n",
            "|2024| 04| 23|\"EY4\"|    2024-04-23|\n",
            "|2024| 04| 22|\"EY4\"|    2024-04-22|\n",
            "|2024| 04| 21|\"EY4\"|    2024-04-21|\n",
            "|2024| 04| 20|\"EY4\"|    2024-04-20|\n",
            "|2024| 04| 19|\"EY4\"|    2024-04-19|\n",
            "|2024| 04| 18|\"EY4\"|    2024-04-18|\n",
            "|2024| 04| 17|\"EY4\"|    2024-04-17|\n",
            "|2024| 04| 16|\"EY4\"|    2024-04-16|\n",
            "|2024| 03| 15|\"EY4\"|    2024-03-15|\n",
            "+----+---+---+-----+--------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max, character_length"
      ],
      "metadata": {
        "id": "JLaqGQ0As8GT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "43112555-a5fc-4d5e-fc47-649f738b6a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'character_length' from 'pyspark.sql.functions' (/content/spark-3.4.0-bin-hadoop3/python/pyspark/sql/functions.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-c616c3d8284c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcharacter_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'character_length' from 'pyspark.sql.functions' (/content/spark-3.4.0-bin-hadoop3/python/pyspark/sql/functions.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnmax_predicate = dfs.select(max(dfs.data_formatada)).first()[0]\n",
        "max_predicate.split('-')"
      ],
      "metadata": {
        "id": "idRaYpF6fWtN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7dced70-f3c3-4031-f312-d1c57ce9b4a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2024', '04', '23']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import character_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "CFdoK2lUMTwK",
        "outputId": "cb2005f8-be10-4059-a861-be319d626744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'character_length' from 'pyspark.sql.functions' (/content/spark-3.4.0-bin-hadoop3/python/pyspark/sql/functions.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-4da050b8e40c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcharacter_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'character_length' from 'pyspark.sql.functions' (/content/spark-3.4.0-bin-hadoop3/python/pyspark/sql/functions.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ano = 'ano'\n",
        "a = dfs.select(max(f.length(f.col(ano)))).first()[0]\n",
        "a"
      ],
      "metadata": {
        "id": "UZbipUi_tFcS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f41cae5-f3da-4bc6-89b5-604d9737a70b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as f"
      ],
      "metadata": {
        "id": "6r7mMIR2KUPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs.schema[ano].dataType"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYK3BcnPMiUX",
        "outputId": "498f462d-91aa-4a96-b7ea-48ed146d0f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringType()"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = dfs.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acAePi3PNL3q",
        "outputId": "a7458f84-8027-4d2c-ab7c-761844d6d6f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- ano: string (nullable = true)\n",
            " |-- mes: string (nullable = true)\n",
            " |-- dia: string (nullable = true)\n",
            " |-- sigla: string (nullable = true)\n",
            " |-- data_formatada: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d= dfs.schema"
      ],
      "metadata": {
        "id": "rcjCnxB-NOqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwqUazTHNSD1",
        "outputId": "9b446c89-32fe-489a-d393-721acf1d9c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('ano', StringType(), True), StructField('mes', StringType(), True), StructField('dia', StringType(), True), StructField('sigla', StringType(), True), StructField('data_formatada', StringType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for value in d:\n",
        "    print(value[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "z8-inAqMNWRW",
        "outputId": "6cf82bf5-c57f-4dcd-ab9c-e67f25523db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'StructField' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-70fbdc075de1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'StructField' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = {\n",
        "    column:dfs.select(\n",
        "    max(\n",
        "        f.length(\n",
        "            f.col(column)\n",
        "        )\n",
        "    )\n",
        ").first()[0] for column in dfs.columns}"
      ],
      "metadata": {
        "id": "h6ivZiYUNa7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOeKu8N0P_Up",
        "outputId": "76cadef7-c081-40db-f417-893adeb93aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ano': 4, 'mes': 2, 'dia': 2, 'sigla': 5, 'data_formatada': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yrovp1SbQAz9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}